version: '3.8'

services:
  # ü§ñ ORA Main Brain
  ora-bot:
    build: .
    container_name: ora-bot
    restart: unless-stopped
    env_file: .env
    volumes:
      - .:/app
      - ./data:/app/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: python main.py
    stdin_open: true # For interactive console
    tty: true

  # üåê NERV Dashboard Backend
  ora-web:
    build: .
    container_name: ora-web
    restart: unless-stopped
    env_file: .env
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    command: uvicorn src.web.app:app --host 0.0.0.0 --port 8000 --reload
    depends_on:
      - ora-bot

  # üß† Local LLM Engine (Optional)
  # Uncomment to run vLLM local inference server
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   container_name: ora-vllm
  #   runtime: nvidia
  #   restart: unless-stopped
  #   environment:
  #     - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   ports:
  #     - "8001:8000"
  #   command: --model Qwen/Qwen2.5-VL-7B-Instruct --gpu-memory-utilization 0.8 --max-model-len 4096
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # üîó Ngrok Tunnel (External Access)
  # ngrok:
  #   image: ngrok/ngrok:latest
  #   container_name: ora-ngrok
  #   command: http ora-web:8000
  #   env_file: .env
  #   ports:
  #     - 4040:4040
